<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Lab 3 - ECE 5160: Fast Robots</title>
  <link rel="stylesheet" href="style.css">
  <script type="text/javascript" async src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <header class="header">
    <div class="nav-container">
      <a href="lab3.html" class="home-link">Lab 3: ToF</a>
      <nav class="nav-menu">
        <a href="index.html" class="home-link">Home</a>
      </nav>
    </div>
  </header>


  <section class="about">
    <h2>Lab Objective</h2>
    <p>
      The purpose of Lab 3, was to get familiar with the ToF sensor and begin planning the placement of electronics on
      our cars.
    </p>
  </section>

  <section class="task1">
    <h2>Lab Set-up/Prelab</h2>
    <p>
      For the prelab I read up on the ToF sensors and began to plan out the positioning of the sensors on
      the car. I filled up the wiring diagram to plan out where I would place my ToF sensors on my car.
      We are using 2 ToF sensors that will help us maximize the obstacle detection range of our robot. I had several
      options for placing the two sensors on my car and therefore used my longer QWIIC connect cables to connect the
      sensors to the breakout board. Not having
      any sensors on the sides might lead to the robot missing obstacles in blind spots, while not having a sensor at
      the back
      would mean the robot would miss obstacles while backing up. After weighing these two options, I decided to
      place the two sensors perpendicular to each other- one on the front of the car and the other on the side.
      With this placement, my car will not be able to detect any obstacles at the back unless turned around.
    </p>
    <div class="image-container">
      <img src="./images/wiring.png" alt="Sensor Addr" class="task-image">
    </div>
    <p>
      By default, the two sensors have the same I2C address, which means that only one of them would measure data. To
      ensure that both sensors measure data, we have to use the XSHUT pin on one of the sensors to manually change its
      address. We connect the XSHUT pin to pin 6 on our Artemis. Then, in void set-up we manually change the address of
      this sensor, which ensures that both sensors have a unique address.
    </p>
  </section>

  <section class="task1">
    <h2>Task 1: Connecting the Artemis/ToF Sensors</h2>
    <div class="image-container">
      <img src="./images/battery_artemis.jpeg" alt="Temp readings" class="task-image">
    </div>
    <div class="image-container">
      <img src="./images/tof_connections.jpeg" alt="Temp readings" class="task-image">
    </div>
    <div class="image-container">
      <img src="./images/all_connections.jpeg" alt="Temp readings" class="task-image">
    </div>
  </section>

  <section class="task1">
    <h2>Task 2: Finding the I2C Channel</h2>
    <p>
      To start collecting data with the ToF sensors, we downloaded the SparkFun VL53L1X 4m laser distance sensor
      library. Then we ran the Example05_wire_I2C.ino code, and saw the Artemis scanning the I2C ports to find the
      sensor. The address of this sensor was found to be 0x29. In the datasheet for the sensor, the address is listed as
      0x52. However, since the rightmost bit is used to identify if data is read/write, it is ignored. Therefore, 0x52
      is shifted to the right, resulting in address 0x29.
    </p>
    <div class="image-container">
      <img src="./images/i2c_addr_scan.png" alt="Temp readings" class="task-image">
    </div>
    </p>
  </section>

  <section class="task1">
    <h2>Task 3: Sensor distance modes</h2>
    <p>
      The ToF sensor library has 3 available sensor modes: short, long and medium (only available with the Polulu
      VL53L1X Library). Short mode has a maximum range of 1.3 m and is the least sensitive to ambient light. Medium mode
      has a maximum range of 3m. Long mode
      has a maximum range of 4m but is the most sensitive to ambient light. The short mode is more accurate and suffers
      from lesser sensor noise. For this lab, I chose to use the short mode to test sensor accuracy. I modified the code
      so that the sensor now uses short mode.
    </p>
    <div class="image-container">
      <img src="./images/dist_mode_code.png" alt="Temp readings" class="task-image">
    </div>

    <p>
      Some of the data collected from the sensor:
    </p>
    <div class="image-container">
      <img src="./images/tof_data_1.png" alt="ToF readings" class="task-image">
    </div>
    <div class="image-container">
      <img src="./images/tof_data_2.png" alt="Temp readings" class="task-image">
    </div>
    <div class="image-container">
      <img src="./images/tof_data_3.png" alt="Temp readings" class="task-image">
    </div>
    <p>
      To test the accuracy of the sensor I collected data for 6 different distance readings ranging from 10 inches to 60
      inches. For each
      distance reading I took 10 data points and then took the mean of those as the final reading. I also calculated the
      standard deviation of the data for each distance reading.
    </p>
    <div class="image-container">
      <img src="./images/tof_measured_actual.png" alt="ToF readings" class="task-image">
    </div>
    <div class="image-container">
      <img src="./images/tof_accuracy.png" alt="Temp readings" class="task-image">
    </div>
    <div class="image-container">
      <img src="./images/tof_precision.png" alt="Temp readings" class="task-image">
    </div>
    <p>
      Python code used to calculate std. dev and mean:
    </p>
    <div class="image-container">
      <img src="./images/sensor_vals_code.png" alt="Temp readings" class="task-image">
    </div>
    <p>
      I also found the ranging time, which is the time it takes between sensor measurements. I found this value to be
      around 89 ms.
    </p>
    <div class="image-container">
      <img src="./images/ranging_time.jpeg" alt="Temp readings" class="task-image">
    </div>
  </section>

  <section class="task1">
    <h2>Task 4: Two Sensors Together</h2>
    <p>
      Now, I connected both the sensors to the breakout board and ran the code again. I had to make sure to use the
      XSHUT pin to manually set the address of one of the sensors. I chose to set this value to 0x30. Printing to the
      serial monitor confirms that the two sensors are collecting data separately.
    </p>
    <div class="image-container">
      <img src="./images/both_tof_sensors_code.png" alt="ToF Speed Tests" class="task-image">
    </div>
    <div class="image-container">
      <img src="./images/tof_two_addresses.png" alt="ToF Speed Tests" class="task-image">
    </div>

    <div class="video-container">
      <iframe width="70%" height="315" src="https://www.youtube.com/embed/uWKT37_UHiU" title="Lab 2 Stunt">
      </iframe>
    </div>
  </section>



  <section class="task1">
    <h2>Task 5: Sensor Speed and Limiting Factor</h2>
    <p>
      I now modified the code to run as fast as possible. Some of the changes I made included removing all print
      statements, only collecting data when the sensors are ready. I also moved the startRanging() and
      stopRanging() statements outside of the data collection for loop.
      I then sent back 500 data points from the arduino to the python script and calculated the time taken for this. As
      a comparison to find the limiting factor, I also recorded the time taken for an empty loop to complete.
      As can be seen below, the loop in which sensor measurements were recorded took 1365 ms while an empty loop took
      close to 0 ms(I stored time data as in int). Therefore, the sensor data measurements seems to be the limiting
      factor for this case.
    </p>
    <div class="image-container">
      <img src="./images/tof_speed_test_.jpeg" alt="Gyro Data" class="task-image">
    </div>
    <div class="image-container">
      <img src="./images/tof_sensor_speed_code.png" alt="Gyro Data" class="task-image">
    </div>
  </section>



  <section class="task1">
    <h2>Task 6: IMU and ToF Data</h2>
    <p>
      Finally, we collected IMU and ToF data simultaneosly over bluetooth and plotted the results vs time. The two plots
      are shown below.
    </p>
    <div class="image-container">
      <img src="./images/python_code_sensor.png" alt="Middle-C" class="task-image">
    </div>
    <div class="image-container">
      <img src="./images/tof_two_sensor_data.jpeg" alt="Middle-C" class="task-image">
    </div>
    <div class="image-container">
      <img src="./images/gyro_roll_filt.png" alt="Middle-C" class="task-image">
    </div>
  </section>


  <section class="task1">
    <h2>Additional 5000 Level Tasks</h2>
    <h4>Types of Infrared Sensors</h4>
    <p>
      Infrared distance sensors come in several varieties, each with distinct advantages and limitations. Proximity IR
      sensors use an emitter-receiver pair to detect object presence when the IR beam is interrupted, offering a simple,
      low-cost solution for applications like garage door safety mechanisms, though they only provide binary detection
      rather than actual distance measurements. Time-of-Flight (ToF) IR sensors calculate precise distances by measuring
      how long it takes for emitted IR light to return after bouncing off objects, providing superior accuracy and range
      but at higher cost and complexity. IR triangulation sensors emit light at an angle and use geometry to determine
      distance based on where reflected light hits a detector, striking a balance between the simplicity of proximity
      sensors and the precision of ToF technology, though their accuracy diminishes with increasing distance.
    </p>

    <h4>Affect of Texture/Color on sensor readings</h4>
    <p>

    </p>
  </section>

  <section class="task1">
    <h2>Conclusion</h2>
    <p>
      This lab was very useful in getting familiar with working with ToF data.
      It also helped in reinforcing the different methods of interacting with and sending/receiving data using both our
      sensors. This will
      be
      very useful in future labs where being able to get accurate distance readings of obstacles will be crucial for our
      robot to safely navigate environments.
    </p>
  </section>

  <section class="task1">
    <h2>References</h2>
    <p>
      I referenced Nila Narayan and Mikayla Lahr's work for Lab 3.
      Lulu Htutt (lh543) and I used the same hardware set-up for our lab but wrote the software code separately.
    </p>
  </section>

</body>

</html>