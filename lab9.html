<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Lab 9 - ECE 5160: Fast Robots</title>
  <link rel="stylesheet" href="style.css">
  <script type="text/javascript" async src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script type="text/javascript" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>

<body>
  <header class="header">
    <div class="nav-container">
      <a href="lab9.html" class="home-link">Lab 9: Mapping</a>
      <nav class="nav-menu">
        <a href="index.html" class="home-link">Home</a>
      </nav>
    </div>
  </header>


  <section class="task1">
    <h3>Lab Objective</h3>
    <p>
      The purpose of Lab 9 was to perform mapping on an environment in the lab using a PID controller. Since I was
      running short on time I did the mappings in a hallway in Rhodes Hall.
    </p>
  </section>

  <section class="task1">
    <h3>PID Controller</h3>
    <p>
      I chose to use option 2 from the lab handout for my orientation control. I set a fixed angle
      incremet of 24 degrees that looped through the angles from 0 degrees to 360 degrees using my PID orientation
      control loop. To achieve this I set up a new case in my arduino code called MAPPING. The robot systematically
      rotates in increments of 24 degrees, using a PID
      controller with preset gain values of KP=0.8, KI=0.001, KD=0 that I found via calibration, to accurately achieve
      each target angle within a 5-degree
      error threshold. At each position, the robot collects 3 distance readings along with timestamps and
      orientation data from the IMU. I got a total of 45 datapoints over 9 angles. Once the full rotation is complete,
      all collected data points: time,
      distance, and yaw measurements—are sent to the Python code via Bluetooth. I then use this data to get the
      mappings.
    </p>
    <div class="image-container">
      <img src="./images/lab9_ble_code1.png" alt="ToF readings" class="task-image">
    </div>
    <div class="image-container">
      <img src="./images/lab9_ble_code2.png" alt="ToF readings" class="task-image">
    </div>
    <div class="image-container">
      <img src="./images/lab9_ble_code3.png" alt="ToF readings" class="task-image">
    </div>
  </section>

  <section class="task1">
    <h3>Getting Data</h3>
    <p>
      I placed the robot at 4 different positions in the hallway and recorded the data at these locations. For each
      location I plotted the ToF vs yaw values on a polar plot.
    </p>
    <div class="image-container">
      <img src="./images/lab9_graph1.png" alt="ToF readings" class="task-image">
    </div>
    <div class="image-container">
      <img src="./images/lab9_graph2.png" alt="ToF readings" class="task-image">
    </div>
    <div class="image-container">
      <img src="./images/lab9_graph3.png" alt="ToF readings" class="task-image">
    </div>
    <div class="image-container">
      <img src="./images/lab9_graph4.png" alt="ToF readings" class="task-image">
    </div>
    <p>
      Once I had generated the four polar plots, for the next step I used the transformation matrices as described in
      lecture to convert the polar plots to cartesian coordinates.
      I had collected data from four positions in the hallways which I labeled as: (0, 0), (0, 3), (0, 6), and (0, 9)
      feet and
      used mathematical transformations to convert these raw distance readings into a coherent spatial map. The
      algorithm applies different transformations based on the angle of each reading – points that should form the left
      wall are placed farther out (around 6 feet) while the right wall points are positioned closer (about 2 feet),
      creating an asymmetric hallway with more space on the left side.
    </p>
    <div>
      <h4>Transformation Mathematics</h4>
      <p>The transformation from polar to Cartesian coordinates involves two key steps:</p>

      <p>First, converting from polar (r, θ) to local Cartesian coordinates:</p>
      <p>
        x<sub>local</sub> = r·cos(θ)<br>
        y<sub>local</sub> = r·sin(θ)
      </p>

      <p>
        Then, applying a transformation matrix to convert local coordinates to global map coordinates:
      </p>

      <p>
        x<sub>global</sub> = cos(α) * x<sub>local</sub> - sin(α) * y<sub>local</sub> + x<sub>robot</sub><br>
        y<sub>global</sub> = sin(α) * x<sub>local</sub> + cos(α) * y<sub>local</sub> + y<sub>robot</sub><br>
      </p>

      <p>
        where (x<sub>robot</sub>, y<sub>robot</sub>) is the robot's position in the global frame, and α is the robot's
        orientation.
      </p>

      <p>In my implementation, I scaled the raw distances (in mm) to feet using the conversion factor:</p>
      <p>
        distance (feet) = distance (mm) / 304.8
      </p>
    </div>
    <p>
      Once this was done I got the following map representing the hallway. I added in lines to the map based on the
      hallway and got the below visualization.
    </p>
    <div class="image-container">
      <img src="./images/map_with_lines.png" alt="ToF readings" class="task-image">
    </div>
  </section>

  <section class="task1">
    <h3>Conclusion</h3>
    <p>
      This lab was quite fun and I enjoyed flipping my robot.
    </p>
  </section>


</body>

</html>